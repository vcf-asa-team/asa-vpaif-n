{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f713d8f-b0ce-43b3-9355-82d2177872b8",
   "metadata": {},
   "source": [
    "Pull the nvidia container from build.nvidia.com or ngc.nvidia.com.  If you are using harbor you can use the proxy cache option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd64a858-ae6d-4a8d-a54c-1d3bcbfa63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name=\"llama-3.2-11b-vision-instruct\"\n",
    "harbor=\"vpaif-harbor.set.lab\"\n",
    "harbor_login=\"admin\"\n",
    "harbor_pwd=\"Harbor12345\"\n",
    "uri=\"$harbor/nvidia-cache/nim/meta/llama-3.2-11b-vision-instruct:latest\"\n",
    "NGC_API_KEY=\"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\"\n",
    "resource_path=\"/opt/data/resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac16d35-0cb4-4515-b0df-e1768529706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from nvidia-cache/nim/meta/llama-3.2-11b-vision-instruct\n",
      "Digest: sha256:c1ecd5153ee61cf75498590899d150e599f13e52b24cc6062459f4b4583d6956\n",
      "Status: Image is up to date for vpaif-harbor.set.lab/nvidia-cache/nim/meta/llama-3.2-11b-vision-instruct:latest\n",
      "vpaif-harbor.set.lab/nvidia-cache/nim/meta/llama-3.2-11b-vision-instruct:latest\n"
     ]
    }
   ],
   "source": [
    "docker pull $uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c7b070-5ad2-4b74-8a9b-e554662b74a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "== NVIDIA Inference Microservice VLM NIM ==\n",
      "===========================================\n",
      "\n",
      "Model: meta/llama-3.2-11b-vision-instruct\n",
      "\n",
      "Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "The use of this model is governed by the NVIDIA AI Foundation Models Community License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/#:~:text=This%20license%20agreement%20(%E2%80%9CAgreement%E2%80%9D,algorithms%2C%20parameters%2C%20configuration%20files%2C).\n",
      "\n",
      "A copy of this license can be found under /opt/nim/LICENSE.\n",
      "\n",
      "SYSTEM INFO\n",
      "- Free GPUs:\n",
      "  -  [2235:10de] (0) NVIDIA A40-4C [current utilization: 0%]\n",
      "MODEL PROFILES\n",
      "- Compatible with system and runnable:\n",
      "  - ef4378d956c8f663391615ba64b117e74005bcb39a168b6254b731c0d77c40fa (vllm-bf16-tp1)\n",
      "  - With LoRA support:\n",
      "- Incompatible with system:\n",
      "  - ab89f816413848c86e311123d2ed98af7bcda0c3624b0a6c4d43704b720585d5 (tensorrt_llm-h100-fp8-tp2-latency)\n",
      "  - a5dcdf7b6790e6084e6bef28018a6f22e578c43c335defec9723c1c5b7232f7a (tensorrt_llm-h200-fp8-tp2-latency)\n",
      "  - e03a703cb8cae61f8e021edaf5e9fe061aaf576f34a9892281d4d621f4877617 (tensorrt_llm-h100-pcie-fp8-tp2-latency)\n",
      "  - fa1e1cbf698be85c0cc56d707f8bc5b17044e091136dae3f8e4be694af727c87 (tensorrt_llm-h100-fp8-tp1-throughput)\n",
      "  - 49ec4c63d948e1176be848da5f76f2a70189a27816c07bd838b795bdc37e8de1 (tensorrt_llm-h200-fp8-tp1-throughput)\n",
      "  - a362292353e444188d097f51280466df62cd218232ec323808696f1bb6a7a6e4 (tensorrt_llm-h100-pcie-fp8-tp1-throughput)\n",
      "  - 5a9f2d4459908cf6c5b5222e31b8df053c00354b5866f6ee3b8de7552a695524 (tensorrt_llm-a10g-bf16-tp8-latency)\n",
      "  - be5af3c968ce6bc45e740edc985fa05dffd3695abb7cc5723407e1f5e3f12c70 (tensorrt_llm-l40s-bf16-tp4-latency)\n",
      "  - ad16e693a234cf8eee85c43dd66ab4502c51ab0bc553af1644477a4f966bf5c6 (tensorrt_llm-a100-bf16-tp2-latency)\n",
      "  - 126d5a664ba4b6b4557d5e0225b51a5e2ffbf9e9909bfe25ed203bec421ea2e5 (tensorrt_llm-h100-bf16-tp2-latency)\n",
      "  - 6122a45133489ec5f764162a7cc4a9d7273dc931b50a62e7184215b1882d02e2 (tensorrt_llm-h200-bf16-tp2-latency)\n",
      "  - ec1def31d19c1b0aae4adbf39747adf09287e84c4f9c77c90d04dd1bf430051a (tensorrt_llm-a100-pcie-bf16-tp2-latency)\n",
      "  - dda862d83589481be7796c14349fc9caf8e0590caccfb724df1981ae5028731c (tensorrt_llm-h100-pcie-bf16-tp2-latency)\n",
      "  - 417611b3f9e2c25db671083acfcfd4c2340f511f3533838fc6366bb47960915c (tensorrt_llm-a10g-bf16-tp4-throughput)\n",
      "  - b16d5969212a8cea632fd6f70928ab514aab835cf217281899564933e6cafa5b (tensorrt_llm-l40s-bf16-tp2-throughput)\n",
      "  - ee1e936b878082dee74574deae5064cc7fba3e11ba155de1198ee544d7c3468a (tensorrt_llm-a100-bf16-tp1-throughput)\n",
      "  - b7aa6bf9d9946de665480a5669bb73f981eab7c4fe43ddf7217b672eb11a003a (tensorrt_llm-h100-bf16-tp1-throughput)\n",
      "  - 1e6e96a98039f9b222f1c593cd5f59d25f709299fdc31e85465f97b9f7b1d3ba (tensorrt_llm-h200-bf16-tp1-throughput)\n",
      "  - 8ea154d03514d05625094bb9e7e024174d0d294bc526287f3a37d1e7f6aa60d5 (tensorrt_llm-a100-pcie-bf16-tp1-throughput)\n",
      "  - fa1730671b21f5791a2a98a63eeacf205f8d4d317156627d28daa67e9e0d6be0 (tensorrt_llm-h100-pcie-bf16-tp1-throughput)\n",
      "  - a3381d3e72e1a9236c494ed422f08194c35ab013d4e2c1aeea7ea9fc6338fa07 (vllm-bf16-tp4)\n",
      "  - b46681d3e72e1a936c494ed422f08194c35ab013d4e2c1aeea7ea9fc6338fa07 (vllm-bf16-tp2)\n"
     ]
    }
   ],
   "source": [
    "docker run -it --rm --name=\"$image_name\" \\\n",
    "    -e NGC_API_KEY=\"$NGC_API_KEY\" \\\n",
    "    \"$uri\" \\\n",
    "    bash -c \"list-model-profiles\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ca405-ca35-4afc-b88e-c6b901cd9ac5",
   "metadata": {},
   "source": [
    "Find the profile id and name in the list that correlates wtih your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6b22b1-5c7d-42c4-8720-9cad6b8ae08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_id=\"ef4378d956c8f663391615ba64b117e74005bcb39a168b6254b731c0d77c40fa\"\n",
    "profile_name=\"vllm-fp16-tp1\"\n",
    "local_model_cache_path=\"$resource_path/$image_name/$profile_name\"_cache\n",
    "local_model_store_path=\"$resource_path/$image_name/$profile_name\"_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710d995-d4eb-4b9e-94e0-0e12456419e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "== NVIDIA Inference Microservice VLM NIM ==\n",
      "===========================================\n",
      "\n",
      "Model: meta/llama-3.2-11b-vision-instruct\n",
      "\n",
      "Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "The use of this model is governed by the NVIDIA AI Foundation Models Community License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/#:~:text=This%20license%20agreement%20(%E2%80%9CAgreement%E2%80%9D,algorithms%2C%20parameters%2C%20configuration%20files%2C).\n",
      "\n",
      "A copy of this license can be found under /opt/nim/LICENSE.\n",
      "\n",
      "INFO 2025-03-29 00:33:06.869 pre_download.py:131] Fetching contents for profile ef4378d956c8f663391615ba64b117e74005bcb39a168b6254b731c0d77c40fa\n",
      "INFO 2025-03-29 00:33:06.872 pre_download.py:138] Copying contents for profile ef4378d956c8f663391615ba64b117e74005bcb39a168b6254b731c0d77c40fa to /model-repo\n"
     ]
    }
   ],
   "source": [
    "docker run -it --rm --name=\"$image_name\" \\\n",
    "    -v \"$local_model_cache_path\":/opt/nim/.cache \\\n",
    "    -v \"$local_model_store_path\":/model-repo \\\n",
    "    -e NGC_API_KEY=\"$NGC_API_KEY\" \\\n",
    "    -u \"$(id -u root)\" \\\n",
    "    \"$uri\" \\\n",
    "    bash -c \"create-model-store --profile $profile_id --model-store /model-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93d045-ec66-4581-bdd0-40a102d638a9",
   "metadata": {},
   "source": [
    "Log into Harbor to upload model to model store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "330e1557-dcd8-4458-accf-628cc6b302ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/vmware/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "\tdocker login \"$harbor\" -u \"$harbor_login\" -p \"$harbor_pwd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d6fe0-5948-4846-b6f7-bebb4d1678d8",
   "metadata": {},
   "source": [
    "Upload the model to Harbor using PAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70d2871e-39f8-4de8-8568-a084216bcd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Digest: sha256:6e7be131d9f8acadbf3fda926ef08ce483078f938c3d4a25b5fbd67f5647ce45\n",
      "Tag: v1\n",
      "To pull this exact model, use\n",
      "   pais models pull --modelStore vpaif-harbor.set.lab/model-store --modelName llama-3.2-11b-vision-instruct/vllm-fp16-tp1 --digest sha256:6e7be131d9f8acadbf3fda926ef08ce483078f938c3d4a25b5fbd67f5647ce45\n"
     ]
    }
   ],
   "source": [
    "cd $local_model_store_path\n",
    "pais models push --modelName $image_name/$profile_name --modelStore $harbor/model-store -t v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
