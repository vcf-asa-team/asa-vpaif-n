services:
  nemollm-inference:
    container_name: inference-ms
    image: ${LLM_IMAGE} # harbor.pse.lab/paif-n/llama-3.1-8b-instruct-pb24h2:1.3.1
    volumes:
    - ${MODEL_DIRECTORY}:/opt/nim/.cache
    user: ${USERID}
    ports:
    - 8000:8000
    expose:
    - '8000'
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
      NIM_MANIFEST_ALLOW_UNSAFE: '1'
      NIM_MODEL_PROFILE: ${NIM_MODEL_PROFILE} #193649a2eb95e821309d6023a2cabb31489d3b690a9973c7ab5d1ff58b0aa7eb
    shm_size: 20gb
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities:
            - gpu
            device_ids:
            - ${LLM_MS_GPU_ID:-0}
    healthcheck:
      test:
      - CMD
      - python3
      - -c
      - import requests; requests.get('http://localhost:8000/v1/health/ready')
      interval: 10s
      timeout: 20s
      retries: 100
    # profiles:
    # - local-nim
    # - nemo-retriever
volumes:
  llm_data:
networks:
  default:
    name: open-webui