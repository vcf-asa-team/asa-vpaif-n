services:
  nemollm-inference:
    container_name: inference-ms
    image: ${LLM_IMAGE}
    volumes:
    - ${MODEL_DIRECTORY}:/opt/nim/.cache
    ports:
    - 8000:8000
    expose:
    - '8000'
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
      NIM_MODEL_PROFILE: ${NIM_MODEL_PROFILE}
    shm_size: 20gb
    env_file:
      - .env
    entrypoint: ${ENTRY:-}
    command: ${VLLM_COMMAND:-}
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities:
            - gpu
            device_ids:
            - ${LLM_MS_GPU_ID:-0}
    healthcheck:
      test:
      - CMD
      - python3
      - -c
      - import requests; requests.get('http://localhost:8000/v1/health/ready')
      interval: 10s
      timeout: 20s
      retries: 100
    # profiles:
    # - local-nim
    # - nemo-retriever
volumes:
  llm_data:
networks:
  default:
    name: nvidia-rag