{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9a62b0",
   "metadata": {},
   "source": [
    "<!-- Banner Image -->\n",
    "<center>\n",
    "    <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/07/rag-representation.jpg\" width=\"75%\">\n",
    "</center>\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\" style=\"color: #76B900;\">NVIDIA AI Workbench</a> •\n",
    "  <a href=\"https://docs.nvidia.com/ai-workbench/\" style=\"color: #76B900;\">User Documentation</a> •\n",
    "  <a href=\"https://docs.nvidia.com/ai-workbench/user-guide/latest/quickstart/example-projects.html\" style=\"color: #76B900;\">Example Projects Catalog</a> •\n",
    "  <a href=\"https://forums.developer.nvidia.com/t/support-workbench-example-project-llama-3-finetune/303411\" style=\"color: #76B900;\"> Problem? Submit a ticket here! </a>\n",
    "</center>\n",
    "\n",
    "# Finetune and deploy an LLM model using SFT and VLLM \n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook, we're going to walk through the flow of using supervised finetuning (SFT) on the Facebook opt-125m model from scratch using the base model and then deploying it using VLLM.  \n",
    "\n",
    "Note that we will be using the base model in this notebook and not the instruct model. Additionally, we will be running through a full finetune with no quantization. This notebook was originally built for 2 A100-80GB GPUs, but the default hyperparameters have since been adjusted to run on 1x A100-80GB. If you're looking for a lighter Llama3-finetune, checkout out the other Llama3 finetuning notebook which uses Direct Preference Optimization.\n",
    "\n",
    "#### Help us make this tutorial better! Please provide feedback on the [NVIDIA Developer Forum](https://forums.developer.nvidia.com/c/ai-data-science/nvidia-ai-workbench/671).\n",
    "\n",
    "A note about running Jupyter Notebooks: Press Shift + Enter to run a cell. A * in the left-hand cell box means the cell is running. A number means it has completed. If your Notebook is acting weird, you can interrupt a too-long process by interrupting the kernel (Kernel tab -> Interrupt Kernel) or even restarting the kernel (Kernel tab -> Restart Kernel). Note restarting the kernel will require you to run everything from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fc88a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/lib/scimath.py:46: RuntimeWarning: divide by zero encountered in log\n",
      "  _ln2 = nx.log(2.0)\n",
      "2025-01-19 13:58:56.229222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-19 13:58:56.636289: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 13:58:58.052483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-19 13:58:58.052616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-19 13:58:58.052632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO 01-19 13:59:01 api_server.py:585] vLLM API server version 0.6.4\n",
      "INFO 01-19 13:59:01 api_server.py:586] args: Namespace(host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=True, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.85, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\n",
      "WARNING 01-19 13:59:12 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 01-19 13:59:12 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 01-19 13:59:12 config.py:791] Possibly too large swap space. 4.00 GiB out of the 9.72 GiB total CPU memory is allocated for the swap space.\n",
      "INFO 01-19 13:59:12 llm_engine.py:249] Initializing an LLM engine (v0.6.4) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 01-19 13:59:13 selector.py:135] Using Flash Attention backend.\n",
      "INFO 01-19 13:59:13 model_runner.py:1072] Starting to load model facebook/opt-125m...\n",
      "INFO 01-19 13:59:13 weight_utils.py:243] Using model weights format ['*.bin']\n",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]\n",
      "\n",
      "INFO 01-19 13:59:14 model_runner.py:1077] Loading model weights took 0.2389 GB\n",
      "INFO 01-19 13:59:14 worker.py:232] Memory profiling results: total_gpu_memory=5.00GiB initial_memory_usage=1.18GiB peak_torch_memory=0.71GiB memory_usage_post_profile=1.22Gib non_torch_memory=0.97GiB kv_cache_size=2.57GiB gpu_memory_utilization=0.85\n",
      "INFO 01-19 13:59:15 gpu_executor.py:113] # GPU blocks: 4671, # CPU blocks: 7281\n",
      "INFO 01-19 13:59:15 gpu_executor.py:117] Maximum concurrency for 2048 tokens per request: 36.49x\n",
      "INFO 01-19 13:59:18 launcher.py:19] Available routes are:\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 01-19 13:59:18 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m16386\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 01-19 13:59:28 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 13:59:38 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 13:59:48 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 13:59:58 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 14:00:08 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:50064 - \"\u001b[1mGET /v1/completions HTTP/1.1\u001b[0m\" \u001b[31m405 Method Not Allowed\u001b[0m\n",
      "INFO 01-19 14:00:18 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 14:00:28 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 14:00:29 logger.py:37] Received request cmpl-9fa35610f36b4cb683e7a6fa22da7597-0: prompt: 'The god father movie is', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [2, 133, 9069, 1150, 1569, 16], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 01-19 14:00:29 async_llm_engine.py:208] Added request cmpl-9fa35610f36b4cb683e7a6fa22da7597-0.\n",
      "INFO 01-19 14:00:30 async_llm_engine.py:176] Finished request cmpl-9fa35610f36b4cb683e7a6fa22da7597-0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:46810 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-19 14:00:38 metrics.py:449] Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 14:00:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 14:01:02 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-19 14:01:14 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "#run HF model\n",
    "!python -O -u -m vllm.entrypoints.openai.api_server \\\n",
    "    --host=0.0.0.0 \\\n",
    "    --port=8000 \\\n",
    "    --model=facebook/opt-125m \\\n",
    "    --tensor-parallel-size=1 \\\n",
    "    --gpu-memory-utilization=0.85 \\\n",
    "    --enforce-eager \\\n",
    "    --disable-frontend-multiprocessing \\\n",
    "    --max-model-len=2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d71aea-c9f9-4a8a-bcac-7e7a5b58e6bf",
   "metadata": {},
   "source": [
    "# Open up a terminal and run\n",
    "\n",
    "*Baseline test:*\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "   -H \"Content-Type: application/json\" \\\n",
    "   -d '{\n",
    "       \"model\": \"facebook/opt-125m\",\n",
    "       \"prompt\": \"The god father movie is\",\n",
    "       \"max_tokens\": 30,\n",
    "       \"temperature\": 0\n",
    "   }'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
